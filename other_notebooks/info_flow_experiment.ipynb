{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d12c01-8b38-4586-b041-7637af3413c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991823d8-9a67-4b70-a0a8-b6aba08a4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from mamba2mini import Mamba2LMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e4e3b-ee97-499c-9b56-8497376df6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_name = \"state-spaces/mamba2-1.3b\"\n",
    "seed = 0\n",
    "n_prompts = 1000\n",
    "n_layers = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf46fce-ad95-48a0-8236-c7450ed915ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set correct caching directories\n",
    "\n",
    "# hf_dir = XXX\n",
    "# tri_dir = YYY\n",
    "# xdg_dir = ZZZ\n",
    "# os.environ['HF_HOME'] = hf_dir\n",
    "# os.environ['TRITON_CACHE_DIR'] = tri_dir\n",
    "# os.environ['XDG_CACHE_HOME'] = xdg_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772ab14-2fe6-4728-aac4-3566978222ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bddfe8-3106-4672-957c-6fd3973f272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_res = pd.read_parquet('entire_results_original.parquet')\n",
    "attn_res = pd.read_parquet('entire_results_attention.parquet')\n",
    "mask = (original_res['hit'] == attn_res['hit']) & (attn_res['hit'] == True)\n",
    "data = attn_res[mask].sample(n_prompts, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247085fd-46d9-44c5-b573-6dd738215619",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analysis Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59da9d-e465-4669-9bac-25c9e79d69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", cache_dir=hf_dir, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7ae5e-0371-4689-8e7f-3575133fa11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mamba2LMHeadModel.from_pretrained(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52c8f1-7182-4863-9c75-f810da8ee02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(seed)\n",
    "model.eval()\n",
    "temperature = 1\n",
    "top_k = 0\n",
    "top_p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe77753-8a11-4085-8576-ee1ce38ad9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/google-research/google-research/blob/master/dissecting_factual_predictions/utils.py \n",
    "def decode_tokens(tokenizer, token_array):\n",
    "    if hasattr(token_array, \"shape\") and len(token_array.shape) > 1:\n",
    "        return [decode_tokens(tokenizer, row) for row in token_array]\n",
    "    return [tokenizer.decode([t]) for t in token_array]\n",
    "\n",
    "def find_token_range(tokenizer, token_array, substring):\n",
    "    \"\"\"Find the tokens corresponding to the given substring in token_array.\"\"\"\n",
    "    toks = decode_tokens(tokenizer, token_array)\n",
    "    whole_string = \"\".join(toks)\n",
    "    char_loc = whole_string.index(substring)\n",
    "    loc = 0\n",
    "    tok_start, tok_end = None, None\n",
    "    for i, t in enumerate(toks):\n",
    "        loc += len(t)\n",
    "        if tok_start is None and loc > char_loc:\n",
    "            tok_start = i\n",
    "        if tok_end is None and loc >= char_loc + len(substring):\n",
    "            tok_end = i + 1\n",
    "            break\n",
    "    return (tok_start, tok_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2b7b0-2074-40b2-9f82-e9e0c4e1959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_eval(temperature, top_k, top_p, prompt_idx, window, block=None):\n",
    "    prompt = data.loc[prompt_idx, 'prompt']\n",
    "    true_word = data.loc[prompt_idx, 'target_true']\n",
    "    base_prob = data.loc[prompt_idx, 'true_prob']\n",
    "    true_token = tokenizer(true_word, return_tensors=\"pt\", padding=True)\n",
    "    true_id = true_token.input_ids.to(device='cpu')\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = tokens.input_ids.to(device=device)\n",
    "    max_new_length = input_ids.shape[1] + 1\n",
    "    last_idx = input_ids.shape[1] - 1\n",
    "    num_to_masks = {}\n",
    "    first_token = False\n",
    "\n",
    "    tok_start, tok_end = find_token_range(tokenizer, input_ids[0], data.loc[prompt_idx, 'subject'])\n",
    "    subject_tokens = list(range(tok_start, tok_end))\n",
    "    if 0 in subject_tokens:\n",
    "        first_token = True\n",
    "    if block not in ('subject', 'relation'):\n",
    "        blocked_idx = [last_idx]\n",
    "    else:\n",
    "        if block == 'subject':\n",
    "            blocked_idx = subject_tokens\n",
    "        else:\n",
    "            blocked_idx = [i for i in range(last_idx + 1) if i not in subject_tokens]\n",
    "        \n",
    "    for layer in window:\n",
    "        for idx in blocked_idx:\n",
    "            if num_to_masks.get(layer) == None:\n",
    "                num_to_masks[layer] = [(last_idx, idx)]\n",
    "            else:\n",
    "                num_to_masks[layer].append((last_idx, idx))\n",
    "    \n",
    "    fn = lambda: model.generate_single(\n",
    "        input_ids=input_ids,\n",
    "        max_new_length=max_new_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token,\n",
    "        attention=True,\n",
    "        num_to_masks=num_to_masks,\n",
    "    )\n",
    "    out = fn()\n",
    "    next_token_probs = out[-1].detach().cpu().numpy()\n",
    "    max_prob = np.max(next_token_probs, axis=1)[0]\n",
    "    true_prob = next_token_probs[0, true_id[:, 0]]\n",
    "    torch.cuda.empty_cache()\n",
    "    return (true_prob == max_prob, (true_prob - base_prob) * 100.0 / base_prob, first_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79e4a8-b3d0-4a20-9bf5-dd06c5d82663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(temperature, top_k, top_p, prompt_indices, windows, block=None, print_period=500):\n",
    "    counts_w_first = np.zeros((len(windows)))\n",
    "    counts_wo_first = np.zeros((len(windows)))\n",
    "    diffs_w_first = np.zeros((len(windows)))\n",
    "    diffs_wo_first = np.zeros((len(windows)))\n",
    "    w_first = 0\n",
    "    for i, window in enumerate(windows):\n",
    "        print('---------------------------------------------------------------')\n",
    "        print(f'Starting window {i}: {window}')\n",
    "        for j, prompt_idx in enumerate(prompt_indices):\n",
    "            hit, diff, first = forward_eval(temperature, top_k, top_p, prompt_idx, window, block)\n",
    "            if first:\n",
    "                if i == 0:\n",
    "                    w_first += 1\n",
    "                counts_w_first[i] += hit\n",
    "                diffs_w_first[i] += diff\n",
    "            else:\n",
    "                counts_wo_first[i] += hit\n",
    "                diffs_wo_first[i] += diff\n",
    "            if (j+1) % print_period == 0:\n",
    "                print(f'Finished prompt {j}')\n",
    "    counts = counts_w_first + counts_wo_first\n",
    "    diffs = diffs_w_first + diffs_wo_first\n",
    "    return (counts / n_prompts, diffs / n_prompts,\n",
    "            counts_w_first / w_first, diffs_w_first / w_first,\n",
    "            counts_wo_first / (n_prompts - w_first), diffs_wo_first / (n_prompts - w_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce36842-7dee-499c-aa19-7795ac4f2417",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiments - no blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab884f6-aa44-4efe-b225-2292ea54c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_indices = list(range(n_prompts))\n",
    "windows = [[]]\n",
    "no_block_acc, no_block_diff, _, _, _, _ = evaluate(temperature, top_k, top_p, prompt_indices, windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde671c-c013-4c81-8b61-9220df39d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_block_acc)\n",
    "print(no_block_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427bcff-ebb7-461b-be63-890360d11ac7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiments - window size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e75845-b97b-488f-a22b-52442934dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 9\n",
    "prompt_indices = list(range(n_prompts))\n",
    "windows = [list(range(i, i + window_size)) for i in range(0, n_layers - window_size + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a232af-93b2-41df-ad6a-88b07873b628",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "## Block last"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d27e2-9129-4e0f-9521-ba1d592d2c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_acc, last_diff, last_wf_acc, last_wf_diff, last_wof_acc, last_wof_diff = evaluate(temperature, top_k, top_p, \n",
    "                                                                                       prompt_indices, windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80caef4e-712b-4b38-90b2-fcc72ee45a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(last_acc)\n",
    "df.to_parquet(f'block_last_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_diff)\n",
    "df.to_parquet(f'block_last_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wf_acc)\n",
    "df.to_parquet(f'block_last_wf_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wf_diff)\n",
    "df.to_parquet(f'block_last_wf_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wof_acc)\n",
    "df.to_parquet(f'block_last_wof_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wof_diff)\n",
    "df.to_parquet(f'block_last_wof_diff_ws={window_size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c26fe6-a511-487a-82e5-c56341506ab3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Block subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59345db-02aa-44f5-bd3e-21895bd2a416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_acc, sub_diff, sub_wf_acc, sub_wf_diff, sub_wof_acc, sub_wof_diff = evaluate(temperature, top_k, top_p, \n",
    "                                                                                 prompt_indices, windows, block='subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259bf5b-0498-48e8-b988-624bf2c15250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sub_acc)\n",
    "df.to_parquet(f'block_subject_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_diff)\n",
    "df.to_parquet(f'block_subject_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wf_acc)\n",
    "df.to_parquet(f'block_subject_wf_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wf_diff)\n",
    "df.to_parquet(f'block_subject_wf_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wof_acc)\n",
    "df.to_parquet(f'block_subject_wof_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wof_diff)\n",
    "df.to_parquet(f'block_subject_wof_diff_ws={window_size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5795a0a-957d-4093-b5fd-0392f45fe165",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Block relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9720999-a5b0-4d76-9419-a963beee2f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rel_acc, rel_diff, rel_wf_acc, rel_wf_diff, rel_wof_acc, rel_wof_diff = evaluate(temperature, top_k, top_p, \n",
    "                                                                                 prompt_indices, windows, block='relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0fa9a-93bd-44f4-ad61-d2c51ed26de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rel_acc)\n",
    "df.to_parquet(f'block_relation_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_diff)\n",
    "df.to_parquet(f'block_relation_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wf_acc)\n",
    "df.to_parquet(f'block_relation_wf_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wf_diff)\n",
    "df.to_parquet(f'block_relation_wf_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wof_acc)\n",
    "df.to_parquet(f'block_relation_wof_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wof_diff)\n",
    "df.to_parquet(f'block_relation_wof_diff_ws={window_size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57709b1f-166d-42b5-8869-5cb665780ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiments - window size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e2c9e-bab7-4393-bfae-58fd954e80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "prompt_indices = list(range(n_prompts))\n",
    "windows = [list(range(i, i + window_size)) for i in range(0, n_layers - window_size + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d4cf2-f94f-4d03-948d-9d2b48ffaaae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "## Block last"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832f9d7-b17a-4d7b-b704-20ea3cf6effb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_acc, last_diff, last_wf_acc, last_wf_diff, last_wof_acc, last_wof_diff = evaluate(temperature, top_k, top_p, \n",
    "                                                                                       prompt_indices, windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03582e-500c-4c0d-9a2f-90dc1ee6932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(last_acc)\n",
    "df.to_parquet(f'block_last_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_diff)\n",
    "df.to_parquet(f'block_last_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wf_acc)\n",
    "df.to_parquet(f'block_last_wf_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wf_diff)\n",
    "df.to_parquet(f'block_last_wf_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wof_acc)\n",
    "df.to_parquet(f'block_last_wof_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(last_wof_diff)\n",
    "df.to_parquet(f'block_last_wof_diff_ws={window_size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196a37e-4a8f-4b83-8b5b-ba637b486762",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Block subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58208397-15bf-4ef5-93c3-3f8615a3bb16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_acc, sub_diff, sub_wf_acc, sub_wf_diff, sub_wof_acc, sub_wof_diff = evaluate(temperature, top_k, top_p, \n",
    "                                                                                 prompt_indices, windows, block='subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3b753-894b-4826-ad0e-0ed8b5eb462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sub_acc)\n",
    "df.to_parquet(f'block_subject_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_diff)\n",
    "df.to_parquet(f'block_subject_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wf_acc)\n",
    "df.to_parquet(f'block_subject_wf_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wf_diff)\n",
    "df.to_parquet(f'block_subject_wf_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wof_acc)\n",
    "df.to_parquet(f'block_subject_wof_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(sub_wof_diff)\n",
    "df.to_parquet(f'block_subject_wof_diff_ws={window_size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a3b26-4b95-419d-8ae9-be9792fcb0a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Block relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabbac2-bbed-4d5d-8ff4-b822660f3ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rel_acc, rel_diff, rel_wf_acc, rel_wf_diff, rel_wof_acc, rel_wof_diff = evaluate(temperature, top_k, top_p, \n",
    "                                                                                 prompt_indices, windows, block='relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b9492-6ebe-41b7-bc2d-7d531d5cadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rel_acc)\n",
    "df.to_parquet(f'block_relation_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_diff)\n",
    "df.to_parquet(f'block_relation_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wf_acc)\n",
    "df.to_parquet(f'block_relation_wf_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wf_diff)\n",
    "df.to_parquet(f'block_relation_wf_diff_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wof_acc)\n",
    "df.to_parquet(f'block_relation_wof_acc_ws={window_size}.parquet')\n",
    "df = pd.DataFrame(rel_wof_diff)\n",
    "df.to_parquet(f'block_relation_wof_diff_ws={window_size}.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
