{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d12c01-8b38-4586-b041-7637af3413c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991823d8-9a67-4b70-a0a8-b6aba08a4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from mamba2mini import Mamba2LMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e4e3b-ee97-499c-9b56-8497376df6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_name = \"state-spaces/mamba2-1.3b\"\n",
    "seed = 0\n",
    "n_layers = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf46fce-ad95-48a0-8236-c7450ed915ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set correct caching directories\n",
    "\n",
    "# hf_dir = XXX\n",
    "# tri_dir = YYY\n",
    "# xdg_dir = ZZZ\n",
    "# os.environ['HF_HOME'] = hf_dir\n",
    "# os.environ['TRITON_CACHE_DIR'] = tri_dir\n",
    "# os.environ['XDG_CACHE_HOME'] = xdg_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772ab14-2fe6-4728-aac4-3566978222ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bddfe8-3106-4672-957c-6fd3973f272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_res = pd.read_parquet('entire_results_original.parquet')\n",
    "attn_res = pd.read_parquet('entire_results_attention.parquet')\n",
    "mask = (original_res['hit'] == attn_res['hit']) & (attn_res['hit'] == True)\n",
    "data = attn_res[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247085fd-46d9-44c5-b573-6dd738215619",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analysis Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59da9d-e465-4669-9bac-25c9e79d69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", cache_dir=hf_dir, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7ae5e-0371-4689-8e7f-3575133fa11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mamba2LMHeadModel.from_pretrained(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52c8f1-7182-4863-9c75-f810da8ee02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(seed)\n",
    "model.eval()\n",
    "temperature = 1\n",
    "top_k = 0\n",
    "top_p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe77753-8a11-4085-8576-ee1ce38ad9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/google-research/google-research/blob/master/dissecting_factual_predictions/utils.py \n",
    "def decode_tokens(tokenizer, token_array):\n",
    "    if hasattr(token_array, \"shape\") and len(token_array.shape) > 1:\n",
    "        return [decode_tokens(tokenizer, row) for row in token_array]\n",
    "    return [tokenizer.decode([t]) for t in token_array]\n",
    "\n",
    "def find_token_range(tokenizer, token_array, substring):\n",
    "    \"\"\"Find the tokens corresponding to the given substring in token_array.\"\"\"\n",
    "    toks = decode_tokens(tokenizer, token_array)\n",
    "    whole_string = \"\".join(toks)\n",
    "    char_loc = whole_string.index(substring)\n",
    "    loc = 0\n",
    "    tok_start, tok_end = None, None\n",
    "    for i, t in enumerate(toks):\n",
    "        loc += len(t)\n",
    "        if tok_start is None and loc > char_loc:\n",
    "            tok_start = i\n",
    "        if tok_end is None and loc >= char_loc + len(substring):\n",
    "            tok_end = i + 1\n",
    "            break\n",
    "    return (tok_start, tok_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2b7b0-2074-40b2-9f82-e9e0c4e1959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_eval(temperature, top_k, top_p, prompt_idx, window):\n",
    "    prompt = data.loc[prompt_idx, 'prompt']\n",
    "    true_word = data.loc[prompt_idx, 'target_true']\n",
    "    true_token = tokenizer(true_word, return_tensors=\"pt\", padding=True)\n",
    "    true_id = true_token.input_ids.to(device='cpu')\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = tokens.input_ids.to(device=device)\n",
    "    max_new_length = input_ids.shape[1] + 1\n",
    "    last_idx = input_ids.shape[1] - 1\n",
    "    probs = np.zeros((input_ids.shape[1]))\n",
    "\n",
    "    for idx in range(input_ids.shape[1]):\n",
    "        num_to_masks = {layer : [(last_idx, idx)] for layer in window}\n",
    "        \n",
    "        fn = lambda: model.generate_single(\n",
    "            input_ids=input_ids,\n",
    "            max_new_length=max_new_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            eos_token_id=tokenizer.eos_token,\n",
    "            attention=True,\n",
    "            num_to_masks=num_to_masks,\n",
    "        )\n",
    "        \n",
    "        out = fn()\n",
    "        next_token_probs = out[-1].detach().cpu().numpy()\n",
    "        probs[idx] = next_token_probs[0, true_id[:, 0]]\n",
    "        torch.cuda.empty_cache()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79e4a8-b3d0-4a20-9bf5-dd06c5d82663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(temperature, top_k, top_p, prompt_indices, windows):\n",
    "    for prompt_idx in prompt_indices:\n",
    "        prob_mat = []\n",
    "        for window in windows:\n",
    "            prob_mat.append(forward_eval(temperature, top_k, top_p, prompt_idx, window))\n",
    "        prob_mat = np.array(prob_mat).T\n",
    "        prompt = data.loc[prompt_idx, 'prompt']\n",
    "        true_word = data.loc[prompt_idx, 'target_true']\n",
    "        base_prob = data.loc[prompt_idx, 'true_prob']\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = tokens.input_ids.to(device=device)\n",
    "        toks = decode_tokens(tokenizer, input_ids[0]) \n",
    "        last_tok = toks[-1]\n",
    "        toks[-1] = toks[-1] + '*'\n",
    "\n",
    "        fontsize = 8\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        ax = sns.heatmap(prob_mat, cmap=\"Purples_r\", cbar=True)\n",
    "        plt.title(f'Intervening on flow to:' + last_tok + f'\\nwindow: {len(windows[0])}, base probability: {round(base_prob, 4)}', \n",
    "                  fontsize=fontsize)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        x_pos = list(range(0, prob_mat.shape[1], 5))\n",
    "        plt.xticks(ticks=np.array(range(0, prob_mat.shape[1], 5)) + 0.5, labels=[str(x) for x in x_pos], \n",
    "                   rotation=0, fontsize=fontsize)\n",
    "        plt.yticks(ticks=np.arange(prob_mat.shape[0]) + 0.5, labels=toks, rotation=0, fontsize=fontsize)\n",
    "        ax.tick_params(axis='both', which='both', length=0)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.ax.set_xlabel(f'p({true_word[1:]})', labelpad=10, fontsize=fontsize)\n",
    "        cbar.locator = plt.MaxNLocator(nbins=5)\n",
    "        cbar.update_ticks()\n",
    "        cbar.ax.tick_params(labelsize=fontsize)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'heatmap_idx={prompt_idx}_ws={window_size}.pdf', format=\"pdf\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581496ad-180b-4c2c-9bef-5cf50c786f48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672f1b8-f0c2-44a7-9344-ca25a35e4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The following prompts are used in [Geva et al. 23'] and are ones for which our core model is correct:\")\n",
    "print('Commerzbank, whose headquarters are in')\n",
    "print('Edvard Grieg, playing the')\n",
    "print('Statistical Package for the Social Sciences was created by')\n",
    "print('The mother tongue of Pietro Mennea is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244949f-e78b-4c15-9593-5c82dd7f9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_indices = [2841, 661, 3124, 2274]\n",
    "window_size = 5\n",
    "windows = [list(range(i, i + window_size)) for i in range(0, n_layers - window_size + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7fc5d-2b75-48f4-b449-eeb5d1b16261",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(temperature, top_k, top_p, prompt_indices, windows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
