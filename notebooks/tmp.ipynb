{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fast_path_available = True\n"
     ]
    }
   ],
   "source": [
    "from transformers import Mamba2ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer3 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='meta-llama/Llama-3.2-1B', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128012: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_bos_token',\n",
       " '_add_eos_token',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_bos_token',\n",
       " 'add_eos_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'legacy',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'update_post_processor',\n",
       " 'use_default_system_prompt',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_file',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 15256], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3(' Islam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='meta-llama/Llama-3.2-1B', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128012: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.split_special_tokens=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 29871, 16427], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2(' Islam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MambaLMHeadModel.__init__() got an unexpected keyword argument 'device_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMambaLMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate-spaces/mamba2-130m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/mamba_ssm/models/mixer_seq_simple.py:290\u001b[0m, in \u001b[0;36mMambaLMHeadModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name, device, dtype, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    288\u001b[0m config_data \u001b[38;5;241m=\u001b[39m load_config_hf(pretrained_model_name)\n",
      "\u001b[1;32m    289\u001b[0m config \u001b[38;5;241m=\u001b[39m MambaConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_data)\n",
      "\u001b[0;32m--> 290\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    291\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(load_state_dict_hf(pretrained_model_name, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype))\n",
      "\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: MambaLMHeadModel.__init__() got an unexpected keyword argument 'device_map'"
     ]
    }
   ],
   "source": [
    "model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba2-130m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Mamba2Model:\n\tsize mismatch for backbone.embeddings.weight: copying a param with shape torch.Size([50288, 768]) from checkpoint, the shape in current model is torch.Size([50277, 4096]).\n\tsize mismatch for backbone.layers.0.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.0.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.0.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.0.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.0.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.0.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.0.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.0.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.0.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.1.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.1.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.1.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.1.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.1.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.1.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.1.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.1.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.2.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.2.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.2.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.2.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.2.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.2.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.2.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.2.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.2.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.3.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.3.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.3.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.3.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.3.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.3.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.3.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.3.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.3.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.4.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.4.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.4.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.4.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.4.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.4.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.4.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.4.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.4.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.5.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.5.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.5.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.5.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.5.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.5.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.5.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.5.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.5.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.6.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.6.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.6.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.6.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.6.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.6.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.6.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.6.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.6.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.7.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.7.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.7.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.7.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.7.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.7.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.7.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.7.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.7.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.8.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.8.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.8.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.8.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.8.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.8.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.8.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.8.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.8.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.9.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.9.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.9.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.9.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.9.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.9.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.9.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.9.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.9.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.10.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.10.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.10.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.10.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.10.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.10.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.10.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.10.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.10.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.11.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.11.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.11.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.11.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.11.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.11.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.11.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.11.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.11.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.12.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.12.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.12.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.12.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.12.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.12.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.12.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.12.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.12.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.13.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.13.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.13.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.13.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.13.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.13.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.13.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.13.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.13.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.14.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.14.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.14.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.14.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.14.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.14.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.14.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.14.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.14.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.15.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.15.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.15.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.15.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.15.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.15.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.15.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.15.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.15.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.16.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.16.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.16.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.16.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.16.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.16.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.16.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.16.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.16.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.17.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.17.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.17.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.17.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.17.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.17.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.17.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.17.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.17.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.18.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.18.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.18.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.18.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.18.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.18.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.18.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.18.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.18.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.19.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.19.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.19.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.19.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.19.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.19.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.19.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.19.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.19.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.20.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.20.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.20.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.20.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.20.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.20.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.20.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.20.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.20.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.21.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.21.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.21.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.21.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.21.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.21.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.21.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.21.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.21.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.22.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.22.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.22.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.22.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.22.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.22.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.22.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.22.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.22.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.23.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.23.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.23.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.23.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.23.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.23.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.23.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.23.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.23.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.norm_f.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate-spaces/mamba2-130m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4007\u001b[0m     (\n\u001b[1;32m   4008\u001b[0m         model,\n\u001b[1;32m   4009\u001b[0m         missing_keys,\n\u001b[1;32m   4010\u001b[0m         unexpected_keys,\n\u001b[1;32m   4011\u001b[0m         mismatched_keys,\n\u001b[1;32m   4012\u001b[0m         offload_index,\n\u001b[1;32m   4013\u001b[0m         error_msgs,\n\u001b[0;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4559\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg:\n\u001b[1;32m   4556\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4558\u001b[0m         )\n\u001b[0;32m-> 4559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4562\u001b[0m     archs \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Mamba2Model:\n\tsize mismatch for backbone.embeddings.weight: copying a param with shape torch.Size([50288, 768]) from checkpoint, the shape in current model is torch.Size([50277, 4096]).\n\tsize mismatch for backbone.layers.0.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.0.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.0.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.0.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.0.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.0.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.0.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.0.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.0.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.1.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.1.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.1.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.1.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.1.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.1.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.1.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.1.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.2.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.2.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.2.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.2.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.2.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.2.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.2.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.2.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.2.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.3.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.3.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.3.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.3.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.3.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.3.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.3.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.3.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.3.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.4.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.4.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.4.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.4.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.4.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.4.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.4.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.4.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.4.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.5.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.5.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.5.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.5.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.5.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.5.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.5.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.5.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.5.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.6.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.6.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.6.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.6.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.6.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.6.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.6.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.6.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.6.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.7.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.7.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.7.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.7.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.7.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.7.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.7.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.7.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.7.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.8.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.8.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.8.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.8.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.8.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.8.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.8.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.8.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.8.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.9.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.9.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.9.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.9.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.9.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.9.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.9.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.9.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.9.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.10.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.10.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.10.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.10.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.10.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.10.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.10.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.10.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.10.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.11.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.11.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.11.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.11.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.11.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.11.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.11.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.11.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.11.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.12.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.12.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.12.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.12.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.12.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.12.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.12.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.12.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.12.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.13.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.13.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.13.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.13.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.13.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.13.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.13.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.13.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.13.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.14.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.14.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.14.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.14.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.14.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.14.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.14.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.14.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.14.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.15.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.15.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.15.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.15.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.15.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.15.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.15.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.15.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.15.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.16.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.16.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.16.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.16.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.16.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.16.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.16.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.16.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.16.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.17.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.17.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.17.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.17.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.17.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.17.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.17.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.17.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.17.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.18.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.18.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.18.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.18.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.18.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.18.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.18.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.18.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.18.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.19.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.19.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.19.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.19.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.19.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.19.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.19.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.19.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.19.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.20.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.20.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.20.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.20.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.20.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.20.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.20.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.20.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.20.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.21.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.21.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.21.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.21.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.21.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.21.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.21.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.21.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.21.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.22.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.22.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.22.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.22.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.22.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.22.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.22.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.22.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.22.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.layers.23.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for backbone.layers.23.mixer.dt_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.23.mixer.A_log: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.23.mixer.D: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.layers.23.mixer.conv1d.weight: copying a param with shape torch.Size([1792, 1, 4]) from checkpoint, the shape in current model is torch.Size([10240, 1, 4]).\n\tsize mismatch for backbone.layers.23.mixer.conv1d.bias: copying a param with shape torch.Size([1792]) from checkpoint, the shape in current model is torch.Size([10240]).\n\tsize mismatch for backbone.layers.23.mixer.in_proj.weight: copying a param with shape torch.Size([3352, 768]) from checkpoint, the shape in current model is torch.Size([18560, 4096]).\n\tsize mismatch for backbone.layers.23.mixer.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for backbone.layers.23.mixer.out_proj.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([4096, 8192]).\n\tsize mismatch for backbone.norm_f.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"state-spaces/mamba2-130m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mamba2ForCausalLM.from_pretrained('mamba2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "state-spaces/mamba2-130M-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/state-spaces/mamba2-130M-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-672ceb76-6d3a49b21e9613de57b24310;063e763d-6c43-4b94-8167-6a75428f2f1f)\n\nRepository Not Found for url: https://huggingface.co/state-spaces/mamba2-130M-hf/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMamba2ForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate-spaces/mamba2-130M-hf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3301\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3300\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3301\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3305\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3311\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3316\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: state-spaces/mamba2-130M-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "model = Mamba2ForCausalLM.from_pretrained('state-spaces/mamba2-130M-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.datasets.download_dataset import (\n",
    "    load_knowns,\n",
    "    load_knowns_pd,\n",
    "    load_splitted_counter_fact,\n",
    "    load_splitted_knowns,\n",
    ")\n",
    "from src.types import DatasetArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(load_splitted_knowns('train1')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da49f7d52e0454fbf1070d537f5024a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = load_knowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_splitted_knowns('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "test      486\n",
       "train1    241\n",
       "train2    241\n",
       "train3    241\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df)['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Antarctica', ' Apple', ' Amazon', ..., ' CBS', ' London', ' HBO'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add space before\n",
    "df.assign(**{'attribute':lambda x: \" \" + x[\"attribute\"]})[\"attribute\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['subject', 'attribute', 'template', 'prediction', 'prompt', 'relation_id', 'known_id', 'original_idx'],\n",
       "    num_rows: 1209\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.datasets.download_dataset import load_dataset\n",
    "from src.types import DATASETS\n",
    "\n",
    "counterfact = pd.DataFrame(load_dataset(DatasetArgs(name=DATASETS.COUNTER_FACT, splits='all')))\n",
    "counterfact_train1 = pd.DataFrame(load_dataset(DatasetArgs(name=DATASETS.COUNTER_FACT, splits='train1')))\n",
    "counterfact_train1_and_train2 = pd.DataFrame(load_dataset(DatasetArgs(name=DATASETS.COUNTER_FACT, splits=['train1', 'train2'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4382, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfact_train1_and_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "known = load_knowns_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating splitted dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4783313d2094ac3bd32a7d434c17037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6046d849094c5b856f57ad078eeeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc319325f0db4a3994d4c619449c1452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5e90c165d64ebb933be13dd0673c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86555aaad06344d8b66ad5dc328a26b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27d52171a904f48967289c56b64918b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counterfact = pd.DataFrame(load_splitted_counter_fact())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 12)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([( 0, ' Europe', 'Europe'), ( 1, ' Islam', 'Islam'),\n",
       "           ( 2, ' Islam', 'Islam'), ( 3, ' Wales', 'Wales'),\n",
       "           ( 4, ' Paris', 'Paris'), ( 5, ' actor', 'actor'),\n",
       "           ( 6, ' Florence', 'Florence'), ( 7, ' Brazil', 'Brazil'),\n",
       "           ( 8, ' Rome', 'Rome'), ( 9, ' London', 'London'),\n",
       "           (10, ' Fiat', 'Fiat'), (11, ' London', 'London'),\n",
       "           (12, ' Apple', 'Apple'), (13, ' Madrid', 'Madrid'),\n",
       "           (14, ' Japan', 'Japan'), (15, ' astronomy', 'astronomy'),\n",
       "           (16, ' Iran', 'Iran'), (17, ' Apple', 'Apple'),\n",
       "           (18, ' Atlanta', 'Atlanta'), (19, ' Islam', 'Islam'),\n",
       "           (20, ' Microsoft', 'Microsoft'), (21, ' London', 'London'),\n",
       "           (22, ' Paris', 'Paris'), (23, ' Apple', 'Apple'),\n",
       "           (24, ' Japan', 'Japan'), (25, ' Toronto', 'Toronto'),\n",
       "           (26, ' Microsoft', 'Microsoft'), (27, ' Avengers', 'Avengers'),\n",
       "           (28, ' Japan', 'Japan'), (29, ' CBS', 'CBS'),\n",
       "           (30, ' France', 'France'), (31, ' Islam', 'Islam'),\n",
       "           (32, ' Apple', 'Apple'), (33, ' Apple', 'Apple'),\n",
       "           (34, ' Athens', 'Athens'), (35, ' Rome', 'Rome'),\n",
       "           (36, ' Paris', 'Paris'), (37, ' Islam', 'Islam'),\n",
       "           (38, ' Berlin', 'Berlin'), (39, ' Boeing', 'Boeing'),\n",
       "           (40, ' Antarctica', 'Antarctica'),\n",
       "           (41, ' Antarctica', 'Antarctica'),\n",
       "           (42, ' aviation', 'aviation'), (43, ' Paris', 'Paris'),\n",
       "           (44, ' Toronto', 'Toronto'), (45, ' Avengers', 'Avengers'),\n",
       "           (46, ' Islam', 'Islam'), (47, ' Japan', 'Japan'),\n",
       "           (48, ' CBS', 'CBS'), (49, ' Japan', 'Japan'),\n",
       "           (50, ' Kiev', 'Kiev'), (51, ' Rome', 'Rome'),\n",
       "           (52, ' piano', 'piano'), (53, ' CBS', 'CBS'),\n",
       "           (54, ' Asia', 'Asia'), (55, ' catcher', 'catcher'),\n",
       "           (56, ' Amazon', 'Amazon'), (57, ' WWE', 'WWE'),\n",
       "           (58, ' Islam', 'Islam'), (59, ' HBO', 'HBO'),\n",
       "           (60, ' Japan', 'Japan'), (61, ' Moscow', 'Moscow'),\n",
       "           (62, ' Cologne', 'Cologne'), (63, ' Microsoft', 'Microsoft'),\n",
       "           (64, ' Amsterdam', 'Athens'), (65, ' Nintendo', 'Nintendo'),\n",
       "           (66, ' piano', 'piano'), (67, ' Honda', 'Honda'),\n",
       "           (68, ' Asia', 'Asia'), (69, ' Italy', 'Italy'),\n",
       "           (70, ' piano', 'piano'), (71, ' Iran', 'Iran'),\n",
       "           (72, ' Avengers', 'Avengers'), (73, ' London', 'London'),\n",
       "           (74, ' Apple', 'Apple'), (75, ' piano', 'piano'),\n",
       "           (76, ' Microsoft', 'Microsoft'), (77, ' Japan', 'Japan'),\n",
       "           (78, ' Cairo', 'Cairo'), (79, ' Spanish', 'Spanish'),\n",
       "           (80, ' Islam', 'Islam'), (81, ' London', 'London'),\n",
       "           (82, ' Paris', 'Paris'), (83, ' WWE', 'WWE')],\n",
       "          dtype=[('index', '<i8'), ('attribute_x', 'O'), ('attribute_y', 'O')])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[['attribute_x', 'attribute_y']].to_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_prefix</th>\n",
       "      <th>relation_suffix</th>\n",
       "      <th>prompt</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>target_false_id</th>\n",
       "      <th>target_true_id</th>\n",
       "      <th>target_true</th>\n",
       "      <th>target_false</th>\n",
       "      <th>subject</th>\n",
       "      <th>original_idx</th>\n",
       "      <th>attribute</th>\n",
       "      <th>template</th>\n",
       "      <th>prediction</th>\n",
       "      <th>known_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td></td>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td>Viking Age belongs to the continent of</td>\n",
       "      <td>P30</td>\n",
       "      <td>Q51</td>\n",
       "      <td>Q46</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Viking Age</td>\n",
       "      <td>17692</td>\n",
       "      <td>Europe</td>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td>Europe, and the Vikings were a seafaring people</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td></td>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td>Pervez Musharraf follows the religion of</td>\n",
       "      <td>P140</td>\n",
       "      <td>Q748</td>\n",
       "      <td>Q432</td>\n",
       "      <td>Islam</td>\n",
       "      <td>Buddhism</td>\n",
       "      <td>Pervez Musharraf</td>\n",
       "      <td>18202</td>\n",
       "      <td>Islam</td>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td>Islam.\\n\\nThe former president of Pakistan,</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td></td>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td>Dildar Ali Naseerabadi follows the religion of</td>\n",
       "      <td>P140</td>\n",
       "      <td>Q5043</td>\n",
       "      <td>Q432</td>\n",
       "      <td>Islam</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>Dildar Ali Naseerabadi</td>\n",
       "      <td>6429</td>\n",
       "      <td>Islam</td>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td>Islam and is a member of the Ahmadiyya</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{}, in</td>\n",
       "      <td></td>\n",
       "      <td>{}, in</td>\n",
       "      <td>Vale of Glamorgan, in</td>\n",
       "      <td>P131</td>\n",
       "      <td>Q1439</td>\n",
       "      <td>Q25</td>\n",
       "      <td>Wales</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Vale of Glamorgan</td>\n",
       "      <td>8550</td>\n",
       "      <td>Wales</td>\n",
       "      <td>{}, in</td>\n",
       "      <td>Wales, has been named the UK's most expensive</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{} was born in</td>\n",
       "      <td></td>\n",
       "      <td>{} was born in</td>\n",
       "      <td>Jacques-Jean Barre was born in</td>\n",
       "      <td>P19</td>\n",
       "      <td>Q2135</td>\n",
       "      <td>Q90</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Winnipeg</td>\n",
       "      <td>Jacques-Jean Barre</td>\n",
       "      <td>14572</td>\n",
       "      <td>Paris</td>\n",
       "      <td>{} was born in</td>\n",
       "      <td>Paris in 1857. He was a member of</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>{} is written in</td>\n",
       "      <td></td>\n",
       "      <td>{} is written in</td>\n",
       "      <td>Mas Canciones is written in</td>\n",
       "      <td>P407</td>\n",
       "      <td>Q7026</td>\n",
       "      <td>Q1321</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Catalan</td>\n",
       "      <td>Mas Canciones</td>\n",
       "      <td>11866</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>{} is written in</td>\n",
       "      <td>Spanish and is the first book in the series.</td>\n",
       "      <td>1117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td></td>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td>Maumoon Abdul Gayoom follows the religion of</td>\n",
       "      <td>P140</td>\n",
       "      <td>Q9268</td>\n",
       "      <td>Q432</td>\n",
       "      <td>Islam</td>\n",
       "      <td>Judaism</td>\n",
       "      <td>Maumoon Abdul Gayoom</td>\n",
       "      <td>19741</td>\n",
       "      <td>Islam</td>\n",
       "      <td>{} follows the religion of</td>\n",
       "      <td>Islam and is a member of the Ahmadiyya</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>{} died in the city of</td>\n",
       "      <td></td>\n",
       "      <td>{} died in the city of</td>\n",
       "      <td>Sir William Dunn, 1st Baronet, of Lakenheath d...</td>\n",
       "      <td>P20</td>\n",
       "      <td>Q5838</td>\n",
       "      <td>Q84</td>\n",
       "      <td>London</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>Sir William Dunn, 1st Baronet, of Lakenheath</td>\n",
       "      <td>11197</td>\n",
       "      <td>London</td>\n",
       "      <td>{} died in the city of</td>\n",
       "      <td>London on the 1st of May, 1864</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>{} was born in</td>\n",
       "      <td></td>\n",
       "      <td>{} was born in</td>\n",
       "      <td>Adolphe d'Ennery was born in</td>\n",
       "      <td>P19</td>\n",
       "      <td>Q1165</td>\n",
       "      <td>Q90</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Adolphe d'Ennery</td>\n",
       "      <td>11099</td>\n",
       "      <td>Paris</td>\n",
       "      <td>{} was born in</td>\n",
       "      <td>Paris in 1874. He was a member of</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>{} is employed by</td>\n",
       "      <td></td>\n",
       "      <td>{} is employed by</td>\n",
       "      <td>Triple H is employed by</td>\n",
       "      <td>P108</td>\n",
       "      <td>Q9531</td>\n",
       "      <td>Q35339</td>\n",
       "      <td>WWE</td>\n",
       "      <td>BBC</td>\n",
       "      <td>Triple H</td>\n",
       "      <td>37</td>\n",
       "      <td>WWE</td>\n",
       "      <td>{} is employed by</td>\n",
       "      <td>WWE as a commentator and host of the WWE Network</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          relation relation_prefix  \\\n",
       "0   {} belongs to the continent of                   \n",
       "1       {} follows the religion of                   \n",
       "2       {} follows the religion of                   \n",
       "3                           {}, in                   \n",
       "4                   {} was born in                   \n",
       "..                             ...             ...   \n",
       "79                {} is written in                   \n",
       "80      {} follows the religion of                   \n",
       "81          {} died in the city of                   \n",
       "82                  {} was born in                   \n",
       "83               {} is employed by                   \n",
       "\n",
       "                   relation_suffix  \\\n",
       "0   {} belongs to the continent of   \n",
       "1       {} follows the religion of   \n",
       "2       {} follows the religion of   \n",
       "3                           {}, in   \n",
       "4                   {} was born in   \n",
       "..                             ...   \n",
       "79                {} is written in   \n",
       "80      {} follows the religion of   \n",
       "81          {} died in the city of   \n",
       "82                  {} was born in   \n",
       "83               {} is employed by   \n",
       "\n",
       "                                               prompt relation_id  \\\n",
       "0              Viking Age belongs to the continent of         P30   \n",
       "1            Pervez Musharraf follows the religion of        P140   \n",
       "2      Dildar Ali Naseerabadi follows the religion of        P140   \n",
       "3                               Vale of Glamorgan, in        P131   \n",
       "4                      Jacques-Jean Barre was born in         P19   \n",
       "..                                                ...         ...   \n",
       "79                        Mas Canciones is written in        P407   \n",
       "80       Maumoon Abdul Gayoom follows the religion of        P140   \n",
       "81  Sir William Dunn, 1st Baronet, of Lakenheath d...         P20   \n",
       "82                       Adolphe d'Ennery was born in         P19   \n",
       "83                            Triple H is employed by        P108   \n",
       "\n",
       "   target_false_id target_true_id target_true   target_false  \\\n",
       "0              Q51            Q46      Europe     Antarctica   \n",
       "1             Q748           Q432       Islam       Buddhism   \n",
       "2            Q5043           Q432       Islam   Christianity   \n",
       "3            Q1439            Q25       Wales          Texas   \n",
       "4            Q2135            Q90       Paris       Winnipeg   \n",
       "..             ...            ...         ...            ...   \n",
       "79           Q7026          Q1321     Spanish        Catalan   \n",
       "80           Q9268           Q432       Islam        Judaism   \n",
       "81           Q5838            Q84      London          Kabul   \n",
       "82           Q1165            Q90       Paris          Bihar   \n",
       "83           Q9531         Q35339         WWE            BBC   \n",
       "\n",
       "                                         subject  original_idx attribute  \\\n",
       "0                                     Viking Age         17692    Europe   \n",
       "1                               Pervez Musharraf         18202     Islam   \n",
       "2                         Dildar Ali Naseerabadi          6429     Islam   \n",
       "3                              Vale of Glamorgan          8550     Wales   \n",
       "4                             Jacques-Jean Barre         14572     Paris   \n",
       "..                                           ...           ...       ...   \n",
       "79                                 Mas Canciones         11866   Spanish   \n",
       "80                          Maumoon Abdul Gayoom         19741     Islam   \n",
       "81  Sir William Dunn, 1st Baronet, of Lakenheath         11197    London   \n",
       "82                              Adolphe d'Ennery         11099     Paris   \n",
       "83                                      Triple H            37       WWE   \n",
       "\n",
       "                          template  \\\n",
       "0   {} belongs to the continent of   \n",
       "1       {} follows the religion of   \n",
       "2       {} follows the religion of   \n",
       "3                           {}, in   \n",
       "4                   {} was born in   \n",
       "..                             ...   \n",
       "79                {} is written in   \n",
       "80      {} follows the religion of   \n",
       "81          {} died in the city of   \n",
       "82                  {} was born in   \n",
       "83               {} is employed by   \n",
       "\n",
       "                                           prediction  known_id  \n",
       "0     Europe, and the Vikings were a seafaring people       462  \n",
       "1         Islam.\\n\\nThe former president of Pakistan,       860  \n",
       "2              Islam and is a member of the Ahmadiyya       492  \n",
       "3       Wales, has been named the UK's most expensive       862  \n",
       "4                   Paris in 1857. He was a member of       815  \n",
       "..                                                ...       ...  \n",
       "79       Spanish and is the first book in the series.      1117  \n",
       "80             Islam and is a member of the Ahmadiyya        90  \n",
       "81                     London on the 1st of May, 1864       509  \n",
       "82                  Paris in 1874. He was a member of       472  \n",
       "83   WWE as a commentator and host of the WWE Network       745  \n",
       "\n",
       "[84 rows x 15 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    merged_df\n",
    "    [['relation', 'relation_prefix', 'relation_suffix', 'prompt',\n",
    "       'relation_id', 'target_false_id', 'target_true_id', 'target_true',\n",
    "       'target_false', 'subject', 'original_idx', 'attribute', 'template',\n",
    "       'prediction', 'known_id']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, on=['prompt', 'relation_id', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 15)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relation': ['relation_suffix']}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_duplicate_columns(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject_x': {0: ' United Kingdom',\n",
       "  3: ' Canton of Fribourg',\n",
       "  9: ' United Kingdom',\n",
       "  18: ' Soviet Air Defence Forces',\n",
       "  30: ' Panama',\n",
       "  36: ' Melitopol',\n",
       "  47: ' Ruokolahti',\n",
       "  49: ' Laurent Lafitte',\n",
       "  50: ' Isabelle Breitman',\n",
       "  51: ' Canton of Fribourg',\n",
       "  55: ' Georges Pompidou',\n",
       "  60: ' Kenya',\n",
       "  62: ' Iran',\n",
       "  66: ' Talmud Torah school',\n",
       "  68: ' Valentin Rasputin',\n",
       "  72: ' Alexandre Auguste Ledru-Rollin',\n",
       "  73: ' Delphine de Girardin',\n",
       "  74: ' Ireland',\n",
       "  84: ' Edward Bulwer-Lytton',\n",
       "  92: ' Nathalie Kosciusko-Morizet',\n",
       "  103: ' Luhansk Oblast'},\n",
       " 'subject_y': {0: 'United Kingdom',\n",
       "  3: 'Canton of Fribourg',\n",
       "  9: 'United Kingdom',\n",
       "  18: 'Soviet Air Defence Forces',\n",
       "  30: 'Panama',\n",
       "  36: 'Melitopol',\n",
       "  47: 'Ruokolahti',\n",
       "  49: 'Laurent Lafitte',\n",
       "  50: 'Isabelle Breitman',\n",
       "  51: 'Canton of Fribourg',\n",
       "  55: 'Georges Pompidou',\n",
       "  60: 'Kenya',\n",
       "  62: 'Iran',\n",
       "  66: 'Talmud Torah school',\n",
       "  68: 'Valentin Rasputin',\n",
       "  72: 'Alexandre Auguste Ledru-Rollin',\n",
       "  73: 'Delphine de Girardin',\n",
       "  74: 'Ireland',\n",
       "  84: 'Edward Bulwer-Lytton',\n",
       "  92: 'Nathalie Kosciusko-Morizet',\n",
       "  103: 'Luhansk Oblast'}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when [['subject_x', 'subject_y']] not equal\n",
    "merged_df.pipe(lambda x: x[x['subject_x'] != x['subject_y']])[['subject_x', 'subject_y']].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_columns(df):\n",
    "    # Initialize an empty dictionary to store duplicate column pairs\n",
    "    duplicate_columns_map = {}\n",
    "    \n",
    "    # Get a list of column names\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Compare each pair of columns\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col1, col2 = columns[i], columns[j]\n",
    "            \n",
    "            # Check if the columns are duplicates\n",
    "            if df[col1].equals(df[col2]):\n",
    "                if col1 not in duplicate_columns_map:\n",
    "                    duplicate_columns_map[col1] = []\n",
    "                duplicate_columns_map[col1].append(col2)\n",
    "    \n",
    "    return duplicate_columns_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relation': ['relation_suffix']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counterfact, known, how='right', on=['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_prefix</th>\n",
       "      <th>relation_suffix</th>\n",
       "      <th>prompt</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>target_false_id</th>\n",
       "      <th>target_true_id</th>\n",
       "      <th>target_true</th>\n",
       "      <th>target_false</th>\n",
       "      <th>subject</th>\n",
       "      <th>original_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td></td>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td>Vinson Massif belongs to the continent of</td>\n",
       "      <td>P30</td>\n",
       "      <td>Q48</td>\n",
       "      <td>Q51</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Vinson Massif</td>\n",
       "      <td>17368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            relation relation_prefix  \\\n",
       "5605  {} belongs to the continent of                   \n",
       "\n",
       "                     relation_suffix  \\\n",
       "5605  {} belongs to the continent of   \n",
       "\n",
       "                                         prompt relation_id target_false_id  \\\n",
       "5605  Vinson Massif belongs to the continent of         P30             Q48   \n",
       "\n",
       "     target_true_id  target_true target_false        subject  original_idx  \n",
       "5605            Q51   Antarctica         Asia  Vinson Massif         17368  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(counterfact\n",
    " .pipe(lambda x: x[x['subject'] == 'Vinson Massif'])\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_known_ids = merged_df[merged_df.duplicated('known_id', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_prefix</th>\n",
       "      <th>relation_suffix</th>\n",
       "      <th>prompt_x</th>\n",
       "      <th>relation_id_x</th>\n",
       "      <th>target_false_id</th>\n",
       "      <th>target_true_id</th>\n",
       "      <th>target_true</th>\n",
       "      <th>target_false</th>\n",
       "      <th>subject</th>\n",
       "      <th>original_idx</th>\n",
       "      <th>attribute</th>\n",
       "      <th>template</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prompt_y</th>\n",
       "      <th>relation_id_y</th>\n",
       "      <th>known_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{}'s capital is</td>\n",
       "      <td></td>\n",
       "      <td>{}'s capital is</td>\n",
       "      <td>Catalonia's capital is</td>\n",
       "      <td>P36</td>\n",
       "      <td>Q84</td>\n",
       "      <td>Q1492</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>London</td>\n",
       "      <td>Catalonia</td>\n",
       "      <td>4998.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td>Europe, and the Spanish government has been t...</td>\n",
       "      <td>Catalonia belongs to the continent of</td>\n",
       "      <td>P30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{} is located in</td>\n",
       "      <td></td>\n",
       "      <td>{} is located in</td>\n",
       "      <td>Catalonia is located in</td>\n",
       "      <td>P17</td>\n",
       "      <td>Q183</td>\n",
       "      <td>Q29</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Catalonia</td>\n",
       "      <td>18240.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td>Europe, and the Spanish government has been t...</td>\n",
       "      <td>Catalonia belongs to the continent of</td>\n",
       "      <td>P30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{} is a part of the continent of</td>\n",
       "      <td></td>\n",
       "      <td>{} is a part of the continent of</td>\n",
       "      <td>Catalonia is a part of the continent of</td>\n",
       "      <td>P30</td>\n",
       "      <td>Q51</td>\n",
       "      <td>Q46</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Catalonia</td>\n",
       "      <td>8283.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>{} belongs to the continent of</td>\n",
       "      <td>Europe, and the Spanish government has been t...</td>\n",
       "      <td>Catalonia belongs to the continent of</td>\n",
       "      <td>P30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{} premieres on</td>\n",
       "      <td></td>\n",
       "      <td>{} premieres on</td>\n",
       "      <td>Alfred Hitchcock Presents premieres on</td>\n",
       "      <td>P449</td>\n",
       "      <td>Q43359</td>\n",
       "      <td>Q13974</td>\n",
       "      <td>NBC</td>\n",
       "      <td>MTV</td>\n",
       "      <td>Alfred Hitchcock Presents</td>\n",
       "      <td>6071.0</td>\n",
       "      <td>NBC</td>\n",
       "      <td>{} debuted on</td>\n",
       "      <td>NBC in January of 1959, and was a huge</td>\n",
       "      <td>Alfred Hitchcock Presents debuted on</td>\n",
       "      <td>P449</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{} was originally aired on</td>\n",
       "      <td></td>\n",
       "      <td>{} was originally aired on</td>\n",
       "      <td>Alfred Hitchcock Presents was originally aired on</td>\n",
       "      <td>P449</td>\n",
       "      <td>Q674608</td>\n",
       "      <td>Q13974</td>\n",
       "      <td>NBC</td>\n",
       "      <td>ITV</td>\n",
       "      <td>Alfred Hitchcock Presents</td>\n",
       "      <td>17354.0</td>\n",
       "      <td>NBC</td>\n",
       "      <td>{} debuted on</td>\n",
       "      <td>NBC in January of 1959, and was a huge</td>\n",
       "      <td>Alfred Hitchcock Presents debuted on</td>\n",
       "      <td>P449</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>{} plays in the position of</td>\n",
       "      <td></td>\n",
       "      <td>{} plays in the position of</td>\n",
       "      <td>Roger Staubach plays in the position of</td>\n",
       "      <td>P413</td>\n",
       "      <td>Q193592</td>\n",
       "      <td>Q622747</td>\n",
       "      <td>quarterback</td>\n",
       "      <td>midfielder</td>\n",
       "      <td>Roger Staubach</td>\n",
       "      <td>5085.0</td>\n",
       "      <td>football</td>\n",
       "      <td>{} professionally plays the sport</td>\n",
       "      <td>of football. He is the only player in NFL</td>\n",
       "      <td>Roger Staubach professionally plays the sport of</td>\n",
       "      <td>P641</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>{}'s profession is an</td>\n",
       "      <td></td>\n",
       "      <td>{}'s profession is an</td>\n",
       "      <td>Pierre Alcover's profession is an</td>\n",
       "      <td>P106</td>\n",
       "      <td>Q82955</td>\n",
       "      <td>Q33999</td>\n",
       "      <td>actor</td>\n",
       "      <td>politician</td>\n",
       "      <td>Pierre Alcover</td>\n",
       "      <td>19369.0</td>\n",
       "      <td>French</td>\n",
       "      <td>The native language of {} is</td>\n",
       "      <td>French.\\n\\nPierre Alcover is a French</td>\n",
       "      <td>The native language of Pierre Alcover is</td>\n",
       "      <td>P103</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>{} is a native speaker of</td>\n",
       "      <td></td>\n",
       "      <td>{} is a native speaker of</td>\n",
       "      <td>Pierre Alcover is a native speaker of</td>\n",
       "      <td>P103</td>\n",
       "      <td>Q1860</td>\n",
       "      <td>Q150</td>\n",
       "      <td>French</td>\n",
       "      <td>English</td>\n",
       "      <td>Pierre Alcover</td>\n",
       "      <td>14391.0</td>\n",
       "      <td>French</td>\n",
       "      <td>The native language of {} is</td>\n",
       "      <td>French.\\n\\nPierre Alcover is a French</td>\n",
       "      <td>The native language of Pierre Alcover is</td>\n",
       "      <td>P103</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>{} holds the position of</td>\n",
       "      <td></td>\n",
       "      <td>{} holds the position of</td>\n",
       "      <td>Gregory XVI holds the position of</td>\n",
       "      <td>P39</td>\n",
       "      <td>Q30185</td>\n",
       "      <td>Q19546</td>\n",
       "      <td>pope</td>\n",
       "      <td>mayor</td>\n",
       "      <td>Gregory XVI</td>\n",
       "      <td>6234.0</td>\n",
       "      <td>pope</td>\n",
       "      <td>{}, whose position is that of</td>\n",
       "      <td>a \"superior\" pope, has been the</td>\n",
       "      <td>Gregory XVI, whose position is that of a \"supe...</td>\n",
       "      <td>P39</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>{} succumbed at</td>\n",
       "      <td></td>\n",
       "      <td>{} succumbed at</td>\n",
       "      <td>Gregory XVI succumbed at</td>\n",
       "      <td>P20</td>\n",
       "      <td>Q1741</td>\n",
       "      <td>Q220</td>\n",
       "      <td>Rome</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>Gregory XVI</td>\n",
       "      <td>5333.0</td>\n",
       "      <td>pope</td>\n",
       "      <td>{}, whose position is that of</td>\n",
       "      <td>a \"superior\" pope, has been the</td>\n",
       "      <td>Gregory XVI, whose position is that of a \"supe...</td>\n",
       "      <td>P39</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              relation relation_prefix  \\\n",
       "6                      {}'s capital is                   \n",
       "7                     {} is located in                   \n",
       "8     {} is a part of the continent of                   \n",
       "19                     {} premieres on                   \n",
       "20          {} was originally aired on                   \n",
       "...                                ...             ...   \n",
       "1293       {} plays in the position of                   \n",
       "1303             {}'s profession is an                   \n",
       "1304         {} is a native speaker of                   \n",
       "1310          {} holds the position of                   \n",
       "1311                   {} succumbed at                   \n",
       "\n",
       "                       relation_suffix  \\\n",
       "6                      {}'s capital is   \n",
       "7                     {} is located in   \n",
       "8     {} is a part of the continent of   \n",
       "19                     {} premieres on   \n",
       "20          {} was originally aired on   \n",
       "...                                ...   \n",
       "1293       {} plays in the position of   \n",
       "1303             {}'s profession is an   \n",
       "1304         {} is a native speaker of   \n",
       "1310          {} holds the position of   \n",
       "1311                   {} succumbed at   \n",
       "\n",
       "                                               prompt_x relation_id_x  \\\n",
       "6                                Catalonia's capital is           P36   \n",
       "7                               Catalonia is located in           P17   \n",
       "8               Catalonia is a part of the continent of           P30   \n",
       "19               Alfred Hitchcock Presents premieres on          P449   \n",
       "20    Alfred Hitchcock Presents was originally aired on          P449   \n",
       "...                                                 ...           ...   \n",
       "1293            Roger Staubach plays in the position of          P413   \n",
       "1303                  Pierre Alcover's profession is an          P106   \n",
       "1304              Pierre Alcover is a native speaker of          P103   \n",
       "1310                  Gregory XVI holds the position of           P39   \n",
       "1311                           Gregory XVI succumbed at           P20   \n",
       "\n",
       "     target_false_id target_true_id   target_true target_false  \\\n",
       "6                Q84          Q1492     Barcelona       London   \n",
       "7               Q183            Q29         Spain      Germany   \n",
       "8                Q51            Q46        Europe   Antarctica   \n",
       "19            Q43359         Q13974           NBC          MTV   \n",
       "20           Q674608         Q13974           NBC          ITV   \n",
       "...              ...            ...           ...          ...   \n",
       "1293         Q193592        Q622747   quarterback   midfielder   \n",
       "1303          Q82955         Q33999         actor   politician   \n",
       "1304           Q1860           Q150        French      English   \n",
       "1310          Q30185         Q19546          pope        mayor   \n",
       "1311           Q1741           Q220          Rome       Vienna   \n",
       "\n",
       "                        subject  original_idx attribute  \\\n",
       "6                     Catalonia        4998.0    Europe   \n",
       "7                     Catalonia       18240.0    Europe   \n",
       "8                     Catalonia        8283.0    Europe   \n",
       "19    Alfred Hitchcock Presents        6071.0       NBC   \n",
       "20    Alfred Hitchcock Presents       17354.0       NBC   \n",
       "...                         ...           ...       ...   \n",
       "1293             Roger Staubach        5085.0  football   \n",
       "1303             Pierre Alcover       19369.0    French   \n",
       "1304             Pierre Alcover       14391.0    French   \n",
       "1310                Gregory XVI        6234.0      pope   \n",
       "1311                Gregory XVI        5333.0      pope   \n",
       "\n",
       "                               template  \\\n",
       "6        {} belongs to the continent of   \n",
       "7        {} belongs to the continent of   \n",
       "8        {} belongs to the continent of   \n",
       "19                        {} debuted on   \n",
       "20                        {} debuted on   \n",
       "...                                 ...   \n",
       "1293  {} professionally plays the sport   \n",
       "1303       The native language of {} is   \n",
       "1304       The native language of {} is   \n",
       "1310      {}, whose position is that of   \n",
       "1311      {}, whose position is that of   \n",
       "\n",
       "                                             prediction  \\\n",
       "6      Europe, and the Spanish government has been t...   \n",
       "7      Europe, and the Spanish government has been t...   \n",
       "8      Europe, and the Spanish government has been t...   \n",
       "19               NBC in January of 1959, and was a huge   \n",
       "20               NBC in January of 1959, and was a huge   \n",
       "...                                                 ...   \n",
       "1293          of football. He is the only player in NFL   \n",
       "1303              French.\\n\\nPierre Alcover is a French   \n",
       "1304              French.\\n\\nPierre Alcover is a French   \n",
       "1310                    a \"superior\" pope, has been the   \n",
       "1311                    a \"superior\" pope, has been the   \n",
       "\n",
       "                                               prompt_y relation_id_y  \\\n",
       "6                 Catalonia belongs to the continent of           P30   \n",
       "7                 Catalonia belongs to the continent of           P30   \n",
       "8                 Catalonia belongs to the continent of           P30   \n",
       "19                 Alfred Hitchcock Presents debuted on          P449   \n",
       "20                 Alfred Hitchcock Presents debuted on          P449   \n",
       "...                                                 ...           ...   \n",
       "1293   Roger Staubach professionally plays the sport of          P641   \n",
       "1303           The native language of Pierre Alcover is          P103   \n",
       "1304           The native language of Pierre Alcover is          P103   \n",
       "1310  Gregory XVI, whose position is that of a \"supe...           P39   \n",
       "1311  Gregory XVI, whose position is that of a \"supe...           P39   \n",
       "\n",
       "      known_id  \n",
       "6            6  \n",
       "7            6  \n",
       "8            6  \n",
       "19          17  \n",
       "20          17  \n",
       "...        ...  \n",
       "1293      1189  \n",
       "1303      1199  \n",
       "1304      1199  \n",
       "1310      1205  \n",
       "1311      1205  \n",
       "\n",
       "[201 rows x 17 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_known_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Some rows were lost",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(merged_df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(df), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSome rows were lost\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Some rows were lost"
     ]
    }
   ],
   "source": [
    "assert len(merged_df) == len(counterfact), 'Some rows were lost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23026, 16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating splitted dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b034d1d33b4d0c96644e28183ae060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2569fa018927454e9c79ee3ae6dea9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64684c729a8741efa001acced6b6df9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deebe1b3aa1488e85b3b4d8acf0a95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88362be9c20e4a67ac207a3d8c5253fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c357e0fc433742e284af52ae20a36c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7100b96f99334578b222e999318250e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(load_splitted_counter_fact(['train2']))\n",
    "df2 = pd.DataFrame(load_splitted_counter_fact(['train2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>attribute</th>\n",
       "      <th>template</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prompt</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>known_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vinson Massif</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>{} is located in the continent</td>\n",
       "      <td>of Antarctica. It is the largest of the three</td>\n",
       "      <td>Vinson Massif is located in the continent of</td>\n",
       "      <td>P30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beats Music</td>\n",
       "      <td>Apple</td>\n",
       "      <td>{} is owned by</td>\n",
       "      <td>Apple, which is also the owner of Beats Elect...</td>\n",
       "      <td>Beats Music is owned by</td>\n",
       "      <td>P127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audible.com</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>{} is owned by</td>\n",
       "      <td>Amazon.com, Inc. or its affiliates.</td>\n",
       "      <td>Audible.com is owned by</td>\n",
       "      <td>P127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Big Bang Theory</td>\n",
       "      <td>CBS</td>\n",
       "      <td>{} premieres on</td>\n",
       "      <td>CBS on September 22.&lt;|endoftext|&gt;</td>\n",
       "      <td>The Big Bang Theory premieres on</td>\n",
       "      <td>P449</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MacApp</td>\n",
       "      <td>Apple</td>\n",
       "      <td>{}, a product created by</td>\n",
       "      <td>Apple to help developers create apps for the ...</td>\n",
       "      <td>MacApp, a product created by</td>\n",
       "      <td>P178</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>Kyle Farnsworth</td>\n",
       "      <td>pitcher</td>\n",
       "      <td>{} plays in the position of</td>\n",
       "      <td>the pitcher, and he's a very good one</td>\n",
       "      <td>Kyle Farnsworth plays in the position of the</td>\n",
       "      <td>P413</td>\n",
       "      <td>1204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>Gregory XVI</td>\n",
       "      <td>pope</td>\n",
       "      <td>{}, whose position is that of</td>\n",
       "      <td>a \"superior\" pope, has been the</td>\n",
       "      <td>Gregory XVI, whose position is that of a \"supe...</td>\n",
       "      <td>P39</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>My Sister Sam</td>\n",
       "      <td>CBS</td>\n",
       "      <td>{} was originally aired on</td>\n",
       "      <td>CBS in the United States. It was a remake</td>\n",
       "      <td>My Sister Sam was originally aired on</td>\n",
       "      <td>P449</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>Devonshire Arms</td>\n",
       "      <td>London</td>\n",
       "      <td>{} is located in</td>\n",
       "      <td>the heart of the city of London, just a</td>\n",
       "      <td>Devonshire Arms is located in the heart of the...</td>\n",
       "      <td>P276</td>\n",
       "      <td>1207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>Ballers</td>\n",
       "      <td>HBO</td>\n",
       "      <td>{} premieres on</td>\n",
       "      <td>HBO on April 14.&lt;|endoftext|&gt;</td>\n",
       "      <td>Ballers premieres on</td>\n",
       "      <td>P449</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  subject   attribute                        template  \\\n",
       "0           Vinson Massif  Antarctica  {} is located in the continent   \n",
       "1             Beats Music       Apple                  {} is owned by   \n",
       "2             Audible.com      Amazon                  {} is owned by   \n",
       "3     The Big Bang Theory         CBS                 {} premieres on   \n",
       "4                  MacApp       Apple        {}, a product created by   \n",
       "...                   ...         ...                             ...   \n",
       "1204      Kyle Farnsworth     pitcher     {} plays in the position of   \n",
       "1205          Gregory XVI        pope   {}, whose position is that of   \n",
       "1206        My Sister Sam         CBS      {} was originally aired on   \n",
       "1207      Devonshire Arms      London                {} is located in   \n",
       "1208              Ballers         HBO                 {} premieres on   \n",
       "\n",
       "                                             prediction  \\\n",
       "0         of Antarctica. It is the largest of the three   \n",
       "1      Apple, which is also the owner of Beats Elect...   \n",
       "2                   Amazon.com, Inc. or its affiliates.   \n",
       "3                     CBS on September 22.<|endoftext|>   \n",
       "4      Apple to help developers create apps for the ...   \n",
       "...                                                 ...   \n",
       "1204              the pitcher, and he's a very good one   \n",
       "1205                    a \"superior\" pope, has been the   \n",
       "1206          CBS in the United States. It was a remake   \n",
       "1207            the heart of the city of London, just a   \n",
       "1208                      HBO on April 14.<|endoftext|>   \n",
       "\n",
       "                                                 prompt relation_id  known_id  \n",
       "0          Vinson Massif is located in the continent of         P30         0  \n",
       "1                               Beats Music is owned by        P127         1  \n",
       "2                               Audible.com is owned by        P127         2  \n",
       "3                      The Big Bang Theory premieres on        P449         3  \n",
       "4                          MacApp, a product created by        P178         4  \n",
       "...                                                 ...         ...       ...  \n",
       "1204       Kyle Farnsworth plays in the position of the        P413      1204  \n",
       "1205  Gregory XVI, whose position is that of a \"supe...         P39      1205  \n",
       "1206              My Sister Sam was originally aired on        P449      1206  \n",
       "1207  Devonshire Arms is located in the heart of the...        P276      1207  \n",
       "1208                               Ballers premieres on        P449      1208  \n",
       "\n",
       "[1209 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_prefix</th>\n",
       "      <th>relation_suffix</th>\n",
       "      <th>prompt</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>target_false_id</th>\n",
       "      <th>target_true_id</th>\n",
       "      <th>target_true</th>\n",
       "      <th>target_false</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{} was created in the country of</td>\n",
       "      <td></td>\n",
       "      <td>{} was created in the country of</td>\n",
       "      <td>tofu was created in the country of</td>\n",
       "      <td>P495</td>\n",
       "      <td>Q31</td>\n",
       "      <td>Q17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>tofu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{}, formulated in</td>\n",
       "      <td></td>\n",
       "      <td>{}, formulated in</td>\n",
       "      <td>roti, formulated in</td>\n",
       "      <td>P495</td>\n",
       "      <td>Q17</td>\n",
       "      <td>Q668</td>\n",
       "      <td>India</td>\n",
       "      <td>Japan</td>\n",
       "      <td>roti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{}, from</td>\n",
       "      <td></td>\n",
       "      <td>{}, from</td>\n",
       "      <td>Bundesautobahn 1, from</td>\n",
       "      <td>P127</td>\n",
       "      <td>Q340</td>\n",
       "      <td>Q183</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>Bundesautobahn 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{} took up work in</td>\n",
       "      <td></td>\n",
       "      <td>{} took up work in</td>\n",
       "      <td>Friedrich Goldmann took up work in</td>\n",
       "      <td>P937</td>\n",
       "      <td>Q220</td>\n",
       "      <td>Q64</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Rome</td>\n",
       "      <td>Friedrich Goldmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{} is produced by</td>\n",
       "      <td></td>\n",
       "      <td>{} is produced by</td>\n",
       "      <td>Nissan R89C is produced by</td>\n",
       "      <td>P176</td>\n",
       "      <td>Q27564</td>\n",
       "      <td>Q20165</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>Nissan R89C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>{} is in</td>\n",
       "      <td></td>\n",
       "      <td>{} is in</td>\n",
       "      <td>Lochaline is in</td>\n",
       "      <td>P131</td>\n",
       "      <td>Q771</td>\n",
       "      <td>Q208279</td>\n",
       "      <td>Highland</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Lochaline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>{} found employment in</td>\n",
       "      <td></td>\n",
       "      <td>{} found employment in</td>\n",
       "      <td>Karl Wittgenstein found employment in</td>\n",
       "      <td>P937</td>\n",
       "      <td>Q585</td>\n",
       "      <td>Q1741</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>Karl Wittgenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>{} is located in</td>\n",
       "      <td></td>\n",
       "      <td>{} is located in</td>\n",
       "      <td>Prey Veng Province is located in</td>\n",
       "      <td>P131</td>\n",
       "      <td>Q1227</td>\n",
       "      <td>Q424</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>Prey Veng Province</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>{} is native to</td>\n",
       "      <td></td>\n",
       "      <td>{} is native to</td>\n",
       "      <td>Abd al-Qadir al-Husayni is native to</td>\n",
       "      <td>P19</td>\n",
       "      <td>Q43301</td>\n",
       "      <td>Q1218</td>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>Abd al-Qadir al-Husayni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>{} plays the</td>\n",
       "      <td></td>\n",
       "      <td>{} plays the</td>\n",
       "      <td>Jim Weider plays the</td>\n",
       "      <td>P1303</td>\n",
       "      <td>Q5994</td>\n",
       "      <td>Q6607</td>\n",
       "      <td>guitar</td>\n",
       "      <td>piano</td>\n",
       "      <td>Jim Weider</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2191 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              relation relation_prefix  \\\n",
       "0     {} was created in the country of                   \n",
       "1                    {}, formulated in                   \n",
       "2                             {}, from                   \n",
       "3                   {} took up work in                   \n",
       "4                    {} is produced by                   \n",
       "...                                ...             ...   \n",
       "2186                          {} is in                   \n",
       "2187            {} found employment in                   \n",
       "2188                  {} is located in                   \n",
       "2189                   {} is native to                   \n",
       "2190                      {} plays the                   \n",
       "\n",
       "                       relation_suffix                                 prompt  \\\n",
       "0     {} was created in the country of     tofu was created in the country of   \n",
       "1                    {}, formulated in                    roti, formulated in   \n",
       "2                             {}, from                 Bundesautobahn 1, from   \n",
       "3                   {} took up work in     Friedrich Goldmann took up work in   \n",
       "4                    {} is produced by             Nissan R89C is produced by   \n",
       "...                                ...                                    ...   \n",
       "2186                          {} is in                        Lochaline is in   \n",
       "2187            {} found employment in  Karl Wittgenstein found employment in   \n",
       "2188                  {} is located in       Prey Veng Province is located in   \n",
       "2189                   {} is native to   Abd al-Qadir al-Husayni is native to   \n",
       "2190                      {} plays the                   Jim Weider plays the   \n",
       "\n",
       "     relation_id target_false_id target_true_id target_true    target_false  \\\n",
       "0           P495             Q31            Q17       Japan         Belgium   \n",
       "1           P495             Q17           Q668       India           Japan   \n",
       "2           P127            Q340           Q183     Germany        Montreal   \n",
       "3           P937            Q220            Q64      Berlin            Rome   \n",
       "4           P176          Q27564         Q20165      Nissan           Dodge   \n",
       "...          ...             ...            ...         ...             ...   \n",
       "2186        P131            Q771        Q208279    Highland   Massachusetts   \n",
       "2187        P937            Q585          Q1741      Vienna            Oslo   \n",
       "2188        P131           Q1227           Q424    Cambodia          Nevada   \n",
       "2189         P19          Q43301          Q1218   Jerusalem          Fresno   \n",
       "2190       P1303           Q5994          Q6607      guitar           piano   \n",
       "\n",
       "                      subject  \n",
       "0                        tofu  \n",
       "1                        roti  \n",
       "2            Bundesautobahn 1  \n",
       "3          Friedrich Goldmann  \n",
       "4                 Nissan R89C  \n",
       "...                       ...  \n",
       "2186                Lochaline  \n",
       "2187        Karl Wittgenstein  \n",
       "2188       Prey Veng Province  \n",
       "2189  Abd al-Qadir al-Husayni  \n",
       "2190               Jim Weider  \n",
       "\n",
       "[2191 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['relation', 'relation_prefix', 'relation_suffix', 'prompt', 'relation_id', 'target_false_id', 'target_true_id', 'target_true', 'target_false', 'subject'],\n",
       "    num_rows: 2191\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, MambaForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MambaForCausalLM.__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMambaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate-spaces/mamba-130M-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/yandex/DL20232024a/nirendy/repos/ADL_2/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3886\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3880\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3881\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3882\u001b[0m )\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3885\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3886\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3888\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3889\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: MambaForCausalLM.__init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "model = MambaForCausalLM.from_pretrained(\n",
    "    \"state-spaces/mamba-130M-hf\", device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.setup_models import get_tokenizer_and_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.types import MODEL_ARCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_tokenizer_and_model(MODEL_ARCH.LLAMA3_2, '1B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddnext(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x MambaBlock(\n",
       "        (norm): MambaRMSNorm(768, eps=1e-05)\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm(768, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MambaForCausalLM.from_pretrained(\n",
    "#     f\"state-spaces/mamba-130M-hf\", device_map='auto'\n",
    "# )\n",
    "\n",
    "model = MambaForCausalLM.from_pretrained(\n",
    "    \"state-spaces/mamba-130M-hf\"\n",
    ")\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517270528\n",
      "555745280\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete the model\n",
    "del model\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "# Empty the cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517270528\n",
      "555745280\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45458527, -0.18060337,  0.66968511,  1.34909529, -0.39096409,\n",
       "       -0.28525401,  1.48713726,  0.79600267, -0.55202203,  0.4141688 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_interp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Example data: N points over T time steps\n",
    "N = 10  # Number of points\n",
    "T = 10  # Number of original time steps\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random initial positions and movements\n",
    "x = np.cumsum(np.random.randn(T, N), axis=0)  # X positions over time\n",
    "y = np.cumsum(np.random.randn(T, N), axis=0)  # Y positions over time\n",
    "\n",
    "# Interpolate between time steps for smoother transitions\n",
    "smooth_factor = 10  # Number of interpolated frames between each time step\n",
    "x_smooth = np.linspace(0, T - 1, T * smooth_factor)\n",
    "x_interp = np.array([np.interp(x_smooth, np.arange(T), x[:, i]) for i in range(N)]).T\n",
    "y_interp = np.array([np.interp(x_smooth, np.arange(T), y[:, i]) for i in range(N)]).T\n",
    "\n",
    "# Create the figure and scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "scat = ax.scatter(x_interp[0], y_interp[0], s=100, c=range(N), cmap='viridis', edgecolor='k')\n",
    "ax.set_xlim(np.min(x_interp) - 1, np.max(x_interp) + 1)\n",
    "ax.set_ylim(np.min(y_interp) - 1, np.max(y_interp) + 1)\n",
    "ax.set_title(\"Smooth Points Movement Over Time\")\n",
    "\n",
    "# Update function for the animation\n",
    "def update(frame):\n",
    "    scat.set_offsets(np.column_stack((x_interp[frame], y_interp[frame])))\n",
    "    return scat,\n",
    "\n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig, update, frames=len(x_interp), interval=50, blit=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    }
   ],
   "source": [
    "ani.save('movement_animation.gif', writer='imagemagick', fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
